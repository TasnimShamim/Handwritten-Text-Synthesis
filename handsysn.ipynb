{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"AGG\")\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 193] %1 is not a valid Win32 application. Error loading \"c:\\Users\\tasni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Dataset\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter\n",
      "File \u001b[1;32mc:\\Users\\tasni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\__init__.py:262\u001b[0m\n\u001b[0;32m    258\u001b[0m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    260\u001b[0m         kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[1;32m--> 262\u001b[0m     \u001b[43m_load_dll_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\tasni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\__init__.py:245\u001b[0m, in \u001b[0;36m_load_dll_libraries\u001b[1;34m()\u001b[0m\n\u001b[0;32m    241\u001b[0m     err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(last_error)\n\u001b[0;32m    242\u001b[0m     err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    244\u001b[0m     )\n\u001b[1;32m--> 245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    247\u001b[0m     is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 193] %1 is not a valid Win32 application. Error loading \"c:\\Users\\tasni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "path = '/data/'\n",
    "with open(path+'sentences.txt', 'r') as f:\n",
    "    texts = f.read().splitlines()\n",
    "\n",
    "strokes = np.load(path+'strokes.npy', allow_pickle=True,encoding='bytes')\n",
    "print(len(texts), len(strokes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_stroke(stroke, save_name=None):\n",
    "    # Plot a single example.\n",
    "    f, ax = pyplot.subplots()\n",
    "\n",
    "    x = np.cumsum(stroke[:, 1])\n",
    "    y = np.cumsum(stroke[:, 2])\n",
    "\n",
    "    size_x = x.max() - x.min() + 1.0\n",
    "    size_y = y.max() - y.min() + 1.0\n",
    "\n",
    "    f.set_size_inches(5.0 * size_x / size_y, 5.0)\n",
    "\n",
    "    cuts = np.where(stroke[:, 0] == 1)[0]\n",
    "    start = 0\n",
    "\n",
    "    for cut_value in cuts:\n",
    "        ax.plot(x[start:cut_value], y[start:cut_value], \"k-\", linewidth=3)\n",
    "        start = cut_value + 1\n",
    "\n",
    "    ax.axis(\"off\")  # equal\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "    if save_name is None:\n",
    "        pyplot.show()\n",
    "    else:\n",
    "        try:\n",
    "            pyplot.savefig(save_name, bbox_inches=\"tight\", pad_inches=0.5)\n",
    "        except Exception:\n",
    "            print(\"Error building image!: \" + save_name)\n",
    "\n",
    "        pyplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(strokes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plot_stroke(strokes[1],'s1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_offset_normalization(data):\n",
    "    \"\"\"\n",
    "       The co-ordinate offsets are normalised to\n",
    "       mean 0, std. dev. 1 over the training set.\n",
    "    \"\"\"\n",
    "    mean = data[:, :, 1:].mean(axis=(0, 1))\n",
    "    data[:, :, 1:] -= mean\n",
    "    std = data[:, :, 1:].std(axis=(0, 1))\n",
    "    data[:, :, 1:] /= std\n",
    "\n",
    "    return mean, std, data\n",
    "\n",
    "\n",
    "def valid_offset_normalization(mean, std, data):\n",
    "    \"\"\"\n",
    "       Data normalization using train set mean and std\n",
    "    \"\"\"\n",
    "    data[:, :, 1:] -= mean\n",
    "    data[:, :, 1:] /= std\n",
    "    return data\n",
    "\n",
    "\n",
    "def data_denormalization(mean, std, data):\n",
    "    \"\"\"\n",
    "       Data denormalization using train set mean and std\n",
    "    \"\"\"\n",
    "    data[:, :, 1:] *= std\n",
    "    data[:, :, 1:] += mean\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def data_normalization(data):\n",
    "    \"\"\"\n",
    "       Data denormalization using train set mean and std\n",
    "    \"\"\"\n",
    "    mean = data[:, 1:].mean(axis=0)\n",
    "    data[:, 1:] -= mean\n",
    "    std = data[:, 1:].std(axis=0)\n",
    "    data[:, 1:] /= std\n",
    "\n",
    "    return mean, std, data\n",
    "\n",
    "\n",
    "def data_processing(data):\n",
    "    \"\"\"\n",
    "       Data denormalization using train set mean and std\n",
    "    \"\"\"\n",
    "    min_xy = data[:, 1:].min(axis=0)\n",
    "    data[:, 1:] -= min_xy\n",
    "    max_xy = data[:, 1:].max(axis=0)\n",
    "    data[:, 1:] /= (max_xy - min_xy)\n",
    "    data[:, 1:] *= 10\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Global:\n",
    "    train_mean = 0.0\n",
    "    train_std = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class HandwritingDataset(Dataset):\n",
    "    \"\"\"Handwriting dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path, split='train', text_req=False, debug=False, max_seq_len=300, data_aug=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_path (string): Path to the data folder.\n",
    "            split (string): train or valid\n",
    "        \"\"\"\n",
    "        self.text_req = text_req\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.data_aug = data_aug\n",
    "\n",
    "        strokes = np.load(data_path + 'strokes.npy', allow_pickle=True, encoding='bytes')\n",
    "        with open(data_path + 'sentences.txt','r') as file:\n",
    "            texts = file.read().splitlines()\n",
    "\n",
    "        # list of length of each stroke in strokes\n",
    "        lengths = [len(stroke) for stroke in strokes]\n",
    "        max_len = np.max(lengths)\n",
    "        n_total = len(strokes)\n",
    "\n",
    "        # Mask\n",
    "        mask_shape = (n_total, max_len)\n",
    "        mask = np.zeros(mask_shape, dtype=np.float32)\n",
    "\n",
    "        # Convert list of str into array of list of chars\n",
    "        char_seqs = [list(char_seq) for char_seq in texts]\n",
    "        #char_seqs = np.asarray(char_seqs)\n",
    "\n",
    "        char_lens = [len(char_seq) for char_seq in char_seqs]\n",
    "        max_char_len = np.max(char_lens)\n",
    "\n",
    "        # char Mask\n",
    "        mask_shape = (n_total, max_char_len)  # (6000,64)\n",
    "        char_mask = np.zeros(mask_shape, dtype=np.float32)\n",
    "\n",
    "        # Input text array\n",
    "        inp_text = np.ndarray((n_total, max_char_len), dtype='<U1')\n",
    "        inp_text[:, :] = ' '\n",
    "\n",
    "        # Convert list of stroke(array) into ndarray of size(n_total, max_len, 3)\n",
    "        data_shape = (n_total, max_len, 3)\n",
    "        data = np.zeros(data_shape, dtype=np.float32)\n",
    "\n",
    "        for i, (seq_len, text_len) in enumerate(zip(lengths, char_lens)):\n",
    "            mask[i, :seq_len] = 1.\n",
    "            data[i, :seq_len] = strokes[i]\n",
    "            char_mask[i, :text_len] = 1.\n",
    "            inp_text[i, :text_len] = char_seqs[i]\n",
    "\n",
    "        # create vocab\n",
    "        self.id_to_char, self.char_to_id = self.build_vocab(inp_text)\n",
    "        self.vocab_size = len(self.id_to_char)\n",
    "\n",
    "        idx_permute = np.random.permutation(n_total)\n",
    "        data = data[idx_permute]\n",
    "        mask = mask[idx_permute]\n",
    "        inp_text = inp_text[idx_permute]\n",
    "        char_mask = char_mask[idx_permute]\n",
    "\n",
    "        if debug:\n",
    "            data = data[:64]\n",
    "            mask = mask[:64]\n",
    "            inp_text = inp_text[:64]\n",
    "            char_mask = char_mask[:64]\n",
    "\n",
    "        n_train = int(0.9 * data.shape[0])\n",
    "        self._data = data\n",
    "        if split == 'train':\n",
    "            self.dataset = data[:n_train]\n",
    "            self.mask = mask[:n_train]\n",
    "            self.texts = inp_text[:n_train]\n",
    "            self.char_mask = char_mask[:n_train]\n",
    "            Global.train_mean, Global.train_std, self.dataset = train_offset_normalization(\n",
    "                self.dataset)\n",
    "\n",
    "        elif split == 'valid':\n",
    "            self.dataset = data[n_train:]\n",
    "            self.mask = mask[n_train:]\n",
    "            self.texts = inp_text[n_train:]\n",
    "            self.char_mask = char_mask[n_train:]\n",
    "            self.dataset = valid_offset_normalization(\n",
    "                Global.train_mean, Global.train_std, self.dataset)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n",
    "    def idx_to_char(self, id_seq):\n",
    "        return np.array([self.id_to_char[id] for id in id_seq])\n",
    "\n",
    "    def char_to_idx(self, char_seq):\n",
    "        return np.array([self.char_to_id[char] for char in char_seq]).astype(np.float32)\n",
    "\n",
    "    def build_vocab(self, texts):\n",
    "        counter = Counter()\n",
    "        for text in texts:\n",
    "            counter.update(text)\n",
    "        unique_char = sorted(counter)\n",
    "        vocab_size = len(unique_char)\n",
    "\n",
    "        id_to_char = dict(zip(np.arange(vocab_size), unique_char))\n",
    "        char_to_id = dict([(v, k) for (k, v) in id_to_char.items()])\n",
    "        return id_to_char, char_to_id\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        mask = torch.from_numpy(self.mask[idx])\n",
    "\n",
    "        if self.text_req:\n",
    "            input_seq = torch.zeros(self.dataset[idx].shape, dtype=torch.float32)\n",
    "            input_seq[1:, :] = torch.from_numpy(self.dataset[idx, :-1, :])\n",
    "\n",
    "            target = torch.from_numpy(self.dataset[idx])\n",
    "            text = torch.from_numpy(self.char_to_idx(self.texts[idx]))\n",
    "            char_mask = torch.from_numpy(self.char_mask[idx])\n",
    "            return (input_seq, target, mask, text, char_mask)\n",
    "        elif self.data_aug:\n",
    "            seq_len = len(mask.nonzero())\n",
    "            start = 0\n",
    "            end = self.max_seq_len\n",
    "\n",
    "            if seq_len > self.max_seq_len:\n",
    "                start = np.random.randint(0, high=seq_len - self.max_seq_len)\n",
    "                end = start + self.max_seq_len\n",
    "\n",
    "            stroke = self.dataset[idx, start:end, :]\n",
    "\n",
    "            input_seq = torch.zeros(stroke.shape, dtype=torch.float32)\n",
    "            input_seq[1:, :] = torch.from_numpy(stroke[:-1, :])\n",
    "\n",
    "            target = torch.from_numpy(stroke)\n",
    "            mask = mask[start:end]\n",
    "\n",
    "            return (input_seq, target, mask)\n",
    "        else:\n",
    "            input_seq = torch.zeros(self.dataset[idx].shape, dtype=torch.float32)\n",
    "            input_seq[1:, :] = torch.from_numpy(self.dataset[idx, :-1, :])\n",
    "            target = torch.from_numpy(self.dataset[idx])\n",
    "            return (input_seq, target, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_path = path\n",
    "traindata = HandwritingDataset(data_path, split='train', text_req=False, debug=False)\n",
    "validdata = HandwritingDataset(data_path, split='valid', text_req=False, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print('Size of train dataset: {}'.format(len(traindata)))\n",
    "print('Size of valid dataset: {}'.format(len(validdata)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.distributions import bernoulli, uniform\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def stable_softmax(X, dim=2):\n",
    "    max_vec = torch.max(X, dim, keepdim=True)\n",
    "    exp_X = torch.exp(X - max_vec[0])\n",
    "    sum_exp_X = torch.sum(exp_X, dim, keepdim=True)\n",
    "    X_hat = exp_X / sum_exp_X\n",
    "    return X_hat\n",
    "\n",
    "\n",
    "def compute_nll_loss(targets, y_hat, mask, M=20):\n",
    "    epsilon = 1e-6\n",
    "    split_sizes = [1] + [20] * 6\n",
    "    y = torch.split(y_hat, split_sizes, dim=2)\n",
    "\n",
    "    eos_logit = y[0].squeeze()\n",
    "    log_mixture_weights = F.log_softmax(y[1], dim=2)\n",
    "\n",
    "    mu_1 = y[2]\n",
    "    mu_2 = y[3]\n",
    "\n",
    "    logstd_1 = y[4]\n",
    "    logstd_2 = y[5]\n",
    "\n",
    "    rho = torch.tanh(y[6])\n",
    "\n",
    "    log_constant = log_mixture_weights - math.log(2 * math.pi) - logstd_1 - \\\n",
    "        logstd_2 - 0.5 * torch.log(epsilon + 1 - rho.pow(2))\n",
    "\n",
    "    x1 = targets[:, :, 1:2]\n",
    "    x2 = targets[:, :, 2:]\n",
    "\n",
    "    std_1 = torch.exp(logstd_1) + epsilon\n",
    "    std_2 = torch.exp(logstd_2) + epsilon\n",
    "\n",
    "    X1 = ((x1 - mu_1) / std_1).pow(2)\n",
    "    X2 = ((x2 - mu_2) / std_2).pow(2)\n",
    "    X1_X2 = 2 * rho * (x1 - mu_1) * (x2 - mu_2) / (std_1 * std_2)\n",
    "\n",
    "    Z = X1 + X2 - X1_X2\n",
    "\n",
    "    X = -Z / (2 * (epsilon + 1 - rho.pow(2)))\n",
    "\n",
    "    log_sum_exp = torch.logsumexp(log_constant + X, 2)\n",
    "    BCE = nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "    loss_t = -log_sum_exp + BCE(eos_logit, targets[:, :, 0])\n",
    "    loss = torch.sum(loss_t * mask)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def sample_from_out_dist(y_hat, bias):\n",
    "    split_sizes = [1] + [20] * 6\n",
    "    y = torch.split(y_hat, split_sizes, dim=0)\n",
    "\n",
    "    eos_prob = torch.sigmoid(y[0])\n",
    "    mixture_weights = stable_softmax(y[1] * (1 + bias), dim=0)\n",
    "    mu_1 = y[2]\n",
    "    mu_2 = y[3]\n",
    "    std_1 = torch.exp(y[4] - bias)\n",
    "    std_2 = torch.exp(y[5] - bias)\n",
    "    correlations = torch.tanh(y[6])\n",
    "\n",
    "    bernoulli_dist = bernoulli.Bernoulli(probs=eos_prob)\n",
    "    eos_sample = bernoulli_dist.sample()\n",
    "\n",
    "    K = torch.multinomial(mixture_weights, 1)\n",
    "\n",
    "    mu_k = y_hat.new_zeros(2)\n",
    "\n",
    "    mu_k[0] = mu_1[K]\n",
    "    mu_k[1] = mu_2[K]\n",
    "    cov = y_hat.new_zeros(2, 2)\n",
    "    cov[0, 0] = std_1[K].pow(2)\n",
    "    cov[1, 1] = std_2[K].pow(2)\n",
    "    cov[0, 1], cov[1, 0] = (\n",
    "        correlations[K] * std_1[K] * std_2[K],\n",
    "        correlations[K] * std_1[K] * std_2[K],\n",
    "    )\n",
    "\n",
    "    x = torch.normal(mean=torch.Tensor([0.0, 0.0]), std=torch.Tensor([1.0, 1.0])).to(\n",
    "        y_hat.device\n",
    "    )\n",
    "    Z = mu_k + torch.mv(cov, x)\n",
    "\n",
    "    sample = y_hat.new_zeros(1, 1, 3)\n",
    "    sample[0, 0, 0] = eos_sample.item()\n",
    "    sample[0, 0, 1:] = Z\n",
    "    return sample\n",
    "\n",
    "\n",
    "def sample_batch_from_out_dist(y_hat, bias):\n",
    "    batch_size = y_hat.shape[0]\n",
    "    split_sizes = [1] + [20] * 6\n",
    "    y = torch.split(y_hat, split_sizes, dim=1)\n",
    "\n",
    "    eos_prob = torch.sigmoid(y[0])\n",
    "    mixture_weights = stable_softmax(y[1] * (1 + bias), dim=1)\n",
    "    mu_1 = y[2]\n",
    "    mu_2 = y[3]\n",
    "    std_1 = torch.exp(y[4] - bias)\n",
    "    std_2 = torch.exp(y[5] - bias)\n",
    "    correlations = torch.tanh(y[6])\n",
    "\n",
    "    bernoulli_dist = bernoulli.Bernoulli(probs=eos_prob)\n",
    "    eos_sample = bernoulli_dist.sample()\n",
    "\n",
    "    K = torch.multinomial(mixture_weights, 1).squeeze()\n",
    "\n",
    "    mu_k = y_hat.new_zeros((y_hat.shape[0], 2))\n",
    "\n",
    "    mu_k[:, 0] = mu_1[torch.arange(batch_size), K]\n",
    "    mu_k[:, 1] = mu_2[torch.arange(batch_size), K]\n",
    "    cov = y_hat.new_zeros(y_hat.shape[0], 2, 2)\n",
    "    cov[:, 0, 0] = std_1[torch.arange(batch_size), K].pow(2)\n",
    "    cov[:, 1, 1] = std_2[torch.arange(batch_size), K].pow(2)\n",
    "    cov[:, 0, 1], cov[:, 1, 0] = (\n",
    "        correlations[torch.arange(batch_size), K]\n",
    "        * std_1[torch.arange(batch_size), K]\n",
    "        * std_2[torch.arange(batch_size), K],\n",
    "        correlations[torch.arange(batch_size), K]\n",
    "        * std_1[torch.arange(batch_size), K]\n",
    "        * std_2[torch.arange(batch_size), K],\n",
    "    )\n",
    "\n",
    "    X = torch.normal(\n",
    "        mean=torch.zeros(batch_size, 2, 1), std=torch.ones(batch_size, 2, 1)\n",
    "    ).to(y_hat.device)\n",
    "    Z = mu_k + torch.matmul(cov, X).squeeze()\n",
    "\n",
    "    sample = y_hat.new_zeros(batch_size, 1, 3)\n",
    "    sample[:, 0, 0:1] = eos_sample\n",
    "    sample[:, 0, 1:] = Z.squeeze()\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class HandWritingSynthesisNet(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size=400, n_layers=3, output_size=121, window_size=77):\n",
    "        super(HandWritingSynthesisNet, self).__init__()\n",
    "        self.vocab_size = window_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        K = 10\n",
    "        self.EOS = False\n",
    "        self._phi = []\n",
    "\n",
    "        self.lstm_1 = nn.LSTM(3 + self.vocab_size, hidden_size, batch_first=True)\n",
    "        self.lstm_2 = nn.LSTM(\n",
    "            3 + self.vocab_size + hidden_size, hidden_size, batch_first=True\n",
    "        )\n",
    "        # self.lstm_3 = nn.LSTM(\n",
    "        #     3 + hidden_size, hidden_size, batch_first=True\n",
    "        # )\n",
    "        self.lstm_3 = nn.LSTM(\n",
    "            3 + self.vocab_size + hidden_size, hidden_size, batch_first=True\n",
    "        )\n",
    "\n",
    "        self.window_layer = nn.Linear(hidden_size, 3 * K)\n",
    "        self.output_layer = nn.Linear(n_layers * hidden_size, output_size)\n",
    "        # self.init_weight()\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        initial_hidden = (\n",
    "            torch.zeros(self.n_layers, batch_size, self.hidden_size, device=device),\n",
    "            torch.zeros(self.n_layers, batch_size, self.hidden_size, device=device),\n",
    "        )\n",
    "        window_vector = torch.zeros(batch_size, 1, self.vocab_size, device=device)\n",
    "        kappa = torch.zeros(batch_size, 10, 1, device=device)\n",
    "        return initial_hidden, window_vector, kappa\n",
    "\n",
    "    def one_hot_encoding(self, text):\n",
    "        N = text.shape[0]\n",
    "        U = text.shape[1]\n",
    "        encoding = text.new_zeros((N, U, self.vocab_size))\n",
    "        for i in range(N):\n",
    "            encoding[i, torch.arange(U), text[i].long()] = 1.0\n",
    "        return encoding\n",
    "\n",
    "    def compute_window_vector(self, mix_params, prev_kappa, text, text_mask, is_map):\n",
    "        encoding = self.one_hot_encoding(text)\n",
    "        mix_params = torch.exp(mix_params)\n",
    "\n",
    "        alpha, beta, kappa = mix_params.split(10, dim=1)\n",
    "\n",
    "        kappa = kappa + prev_kappa\n",
    "        prev_kappa = kappa\n",
    "\n",
    "        u = torch.arange(text.shape[1], dtype=torch.float32, device=text.device)\n",
    "\n",
    "        phi = torch.sum(alpha * torch.exp(-beta * (kappa - u).pow(2)), dim=1)\n",
    "        if phi[0, -1] > torch.max(phi[0, :-1]):\n",
    "            self.EOS = True\n",
    "        phi = (phi * text_mask).unsqueeze(2)\n",
    "        if is_map:\n",
    "            self._phi.append(phi.squeeze(dim=2).unsqueeze(1))\n",
    "\n",
    "        window_vec = torch.sum(phi * encoding, dim=1, keepdim=True)\n",
    "        return window_vec, prev_kappa\n",
    "\n",
    "    def init_weight(self):\n",
    "        k = math.sqrt(1.0 / self.hidden_size)\n",
    "        for param in self.lstm_1.parameters():\n",
    "            nn.init.uniform_(param, a=-k, b=k)\n",
    "\n",
    "        for param in self.lstm_2.parameters():\n",
    "            nn.init.uniform_(param, a=-k, b=k)\n",
    "\n",
    "        for param in self.lstm_3.parameters():\n",
    "            nn.init.uniform_(param, a=-k, b=k)\n",
    "\n",
    "        nn.init.uniform_(self.window_layer.weight, a=-0.01, b=0.01)\n",
    "        nn.init.constant_(self.window_layer.bias, 0.0)\n",
    "\n",
    "        nn.init.uniform_(self.output_layer.weight, a=-0.1, b=0.1)\n",
    "        nn.init.constant_(self.output_layer.bias, 0.0)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        inputs,\n",
    "        text,\n",
    "        text_mask,\n",
    "        initial_hidden,\n",
    "        prev_window_vec,\n",
    "        prev_kappa,\n",
    "        is_map=False,\n",
    "    ):\n",
    "\n",
    "        hid_1 = []\n",
    "        window_vec = []\n",
    "\n",
    "        state_1 = (initial_hidden[0][0:1], initial_hidden[1][0:1])\n",
    "\n",
    "        for t in range(inputs.shape[1]):\n",
    "            inp = torch.cat((inputs[:, t: t + 1, :], prev_window_vec), dim=2)\n",
    "\n",
    "            hid_1_t, state_1 = self.lstm_1(inp, state_1)\n",
    "            hid_1.append(hid_1_t)\n",
    "\n",
    "            mix_params = self.window_layer(hid_1_t)\n",
    "            window, kappa = self.compute_window_vector(\n",
    "                mix_params.squeeze(dim=1).unsqueeze(2),\n",
    "                prev_kappa,\n",
    "                text,\n",
    "                text_mask,\n",
    "                is_map,\n",
    "            )\n",
    "\n",
    "            prev_window_vec = window\n",
    "            prev_kappa = kappa\n",
    "            window_vec.append(window)\n",
    "\n",
    "        hid_1 = torch.cat(hid_1, dim=1)\n",
    "        window_vec = torch.cat(window_vec, dim=1)\n",
    "\n",
    "        inp = torch.cat((inputs, hid_1, window_vec), dim=2)\n",
    "        state_2 = (initial_hidden[0][1:2], initial_hidden[1][1:2])\n",
    "\n",
    "        hid_2, state_2 = self.lstm_2(inp, state_2)\n",
    "        inp = torch.cat((inputs, hid_2, window_vec), dim=2)\n",
    "        # inp = torch.cat((inputs, hid_2), dim=2)\n",
    "        state_3 = (initial_hidden[0][2:], initial_hidden[1][2:])\n",
    "\n",
    "        hid_3, state_3 = self.lstm_3(inp, state_3)\n",
    "\n",
    "        inp = torch.cat([hid_1, hid_2, hid_3], dim=2)\n",
    "        y_hat = self.output_layer(inp)\n",
    "\n",
    "        return y_hat, [state_1, state_2, state_3], window_vec, prev_kappa\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        inp,\n",
    "        text,\n",
    "        text_mask,\n",
    "        prime_text,\n",
    "        prime_mask,\n",
    "        hidden,\n",
    "        window_vector,\n",
    "        kappa,\n",
    "        bias,\n",
    "        is_map=False,\n",
    "        prime=False,\n",
    "    ):\n",
    "        seq_len = 0\n",
    "        gen_seq = []\n",
    "        with torch.no_grad():\n",
    "            batch_size = inp.shape[0]\n",
    "            print(\"batch_size:\", batch_size)\n",
    "            if prime:\n",
    "                y_hat, state, window_vector, kappa = self.forward(\n",
    "                    inp, prime_text, prime_mask, hidden, window_vector, kappa, is_map\n",
    "                )\n",
    "\n",
    "                _hidden = torch.cat([s[0] for s in state], dim=0)\n",
    "                _cell = torch.cat([s[1] for s in state], dim=0)\n",
    "                # last time step hidden state\n",
    "                hidden = (_hidden, _cell)\n",
    "                # # last time step window vector\n",
    "                # window_vector = window_vector[:, -1:, :]\n",
    "                # # last time step output vector\n",
    "                # y_hat = y_hat[:, -1, :]\n",
    "                # # y_hat = y_hat.squeeze()\n",
    "                # Z = sample_from_out_dist(y_hat, bias)\n",
    "                # inp = Z\n",
    "                # gen_seq.append(Z)\n",
    "                self.EOS = False\n",
    "                inp = inp.new_zeros(batch_size, 1, 3)\n",
    "                _, window_vector, kappa = self.init_hidden(batch_size, inp.device)\n",
    "\n",
    "            while not self.EOS and seq_len < 2000:\n",
    "                y_hat, state, window_vector, kappa = self.forward(\n",
    "                    inp, text, text_mask, hidden, window_vector, kappa, is_map\n",
    "                )\n",
    "\n",
    "                _hidden = torch.cat([s[0] for s in state], dim=0)\n",
    "                _cell = torch.cat([s[1] for s in state], dim=0)\n",
    "                hidden = (_hidden, _cell)\n",
    "                # for batch sampling\n",
    "                # y_hat = y_hat.squeeze(dim=1)\n",
    "                # Z = sample_batch_from_out_dist(y_hat, bias)\n",
    "                y_hat = y_hat.squeeze()\n",
    "                Z = sample_from_out_dist(y_hat, bias)\n",
    "                inp = Z\n",
    "                gen_seq.append(Z)\n",
    "\n",
    "                seq_len += 1\n",
    "\n",
    "        gen_seq = torch.cat(gen_seq, dim=1)\n",
    "        gen_seq = gen_seq.cpu().numpy()\n",
    "\n",
    "        print(\"EOS:\", self.EOS)\n",
    "        print(\"seq_len:\", seq_len)\n",
    "\n",
    "        return gen_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 400\n",
    "n_layers = 3\n",
    "batch_size = 32\n",
    "step_size = 100\n",
    "n_epochs = 50\n",
    "lr = 0.001\n",
    "patience = 15\n",
    "data_path = path\n",
    "save_path = \"./logs/\"\n",
    "data_aug = False\n",
    "debug = False\n",
    "seed = 212\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Fix random seed\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = HandwritingDataset(data_path, split=\"train\", text_req=True, debug=debug, data_aug=data_aug)\n",
    "valid_dataset = HandwritingDataset(data_path, split=\"valid\", text_req=True, debug=debug, data_aug=data_aug)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = HandWritingSynthesisNet(\n",
    "    hidden_size=hidden_size,\n",
    "    n_layers=n_layers,\n",
    "    output_size=121,\n",
    "    window_size=train_dataset.vocab_size,\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_conditional_sequence(\n",
    "    model_path,\n",
    "    char_seq,\n",
    "    device,\n",
    "    char_to_id,\n",
    "    idx_to_char,\n",
    "    bias,\n",
    "    prime,\n",
    "    prime_seq,\n",
    "    real_text,\n",
    "    is_map,\n",
    "    batch_size=1,\n",
    "):\n",
    "    model = HandWritingSynthesisNet(window_size=len(char_to_id))\n",
    "    print(\"Vocab size: \", len(char_to_id))\n",
    "    # load the best model\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "    # Print model's state_dict\n",
    "    # print(f\"Model's state_dict:\")\n",
    "    # for param_tensor in model.state_dict():\n",
    "    #     print(f\"{param_tensor}\\t {model.state_dict()[param_tensor]}\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # initial input\n",
    "    if prime:\n",
    "        inp = prime_seq\n",
    "        real_seq = np.array(list(real_text))\n",
    "        idx_arr = [char_to_id[char] for char in real_seq]\n",
    "        prime_text = np.array([idx_arr for i in range(batch_size)]).astype(np.float32)\n",
    "        prime_text = torch.from_numpy(prime_text).to(device)\n",
    "        prime_mask = torch.ones(prime_text.shape).to(device)\n",
    "    else:\n",
    "        prime_text = None\n",
    "        prime_mask = None\n",
    "        inp = torch.zeros(batch_size, 1, 3).to(device)\n",
    "\n",
    "    char_seq = np.array(list(char_seq + \"  \"))\n",
    "    print(\"\".join(char_seq))\n",
    "    text = np.array(\n",
    "        [[char_to_id[char] for char in char_seq] for i in range(batch_size)]\n",
    "    ).astype(np.float32)\n",
    "    text = torch.from_numpy(text).to(device)\n",
    "\n",
    "    text_mask = torch.ones(text.shape).to(device)\n",
    "\n",
    "    hidden, window_vector, kappa = model.init_hidden(batch_size, device)\n",
    "\n",
    "    print(\"Generating sequence....\")\n",
    "    gen_seq = model.generate(\n",
    "        inp,\n",
    "        text,\n",
    "        text_mask,\n",
    "        prime_text,\n",
    "        prime_mask,\n",
    "        hidden,\n",
    "        window_vector,\n",
    "        kappa,\n",
    "        bias,\n",
    "        is_map,\n",
    "        prime=prime,\n",
    "    )\n",
    "\n",
    "    length = len(text_mask.nonzero())\n",
    "    print(\"Input seq: \", \"\".join(idx_to_char(text[0].detach().cpu().numpy()))[:length])\n",
    "    print(\"Length of input sequence: \", text[0].shape[0])\n",
    "\n",
    "    if is_map:\n",
    "        phi = torch.cat(model._phi, dim=1).cpu().numpy()\n",
    "        phi = phi[0].T\n",
    "    else:\n",
    "        phi = []\n",
    "\n",
    "    return gen_seq, phi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(model, optimizer, epoch, train_loader, device):\n",
    "    model.train()\n",
    "    avg_loss = 0.0\n",
    "\n",
    "    for mini_batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        inputs, targets, mask, text, text_mask = mini_batch\n",
    "        inputs, targets, mask = inputs.to(device), targets.to(device), mask.to(device)\n",
    "        text, text_mask = text.to(device), text_mask.to(device)\n",
    "\n",
    "        batch_size = inputs.shape[0]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        initial_hidden, window_vector, kappa = model.init_hidden(batch_size, device)\n",
    "        y_hat, state, window_vector, kappa = model.forward(\n",
    "            inputs, text, text_mask, initial_hidden, window_vector, kappa\n",
    "        )\n",
    "\n",
    "        loss = compute_nll_loss(targets, y_hat, mask)\n",
    "        y_hat.register_hook(lambda grad: torch.clamp(grad, -100, 100))\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_value_(model.lstm_1.parameters(), 10)\n",
    "        nn.utils.clip_grad_value_(model.lstm_2.parameters(), 10)\n",
    "        nn.utils.clip_grad_value_(model.lstm_3.parameters(), 10)\n",
    "        nn.utils.clip_grad_value_(model.window_layer.parameters(), 10)\n",
    "\n",
    "        optimizer.step()\n",
    "        avg_loss += loss.item()\n",
    "\n",
    "    avg_loss /= len(train_loader.dataset)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def validation(model, valid_loader, device, epoch):\n",
    "    model.eval()\n",
    "    avg_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for mini_batch in tqdm(valid_loader, desc=f\"Validation Epoch {epoch+1}\"):\n",
    "            inputs, targets, mask, text, text_mask = mini_batch\n",
    "            inputs, targets, mask = inputs.to(device), targets.to(device), mask.to(device)\n",
    "            text, text_mask = text.to(device), text_mask.to(device)\n",
    "\n",
    "            batch_size = inputs.shape[0]\n",
    "            initial_hidden, window_vector, kappa = model.init_hidden(batch_size, device)\n",
    "            y_hat, state, window_vector, kappa = model.forward(\n",
    "                inputs, text, text_mask, initial_hidden, window_vector, kappa\n",
    "            )\n",
    "\n",
    "            loss = compute_nll_loss(targets, y_hat, mask)\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "    avg_loss /= len(valid_loader.dataset)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def train(model, train_loader, valid_loader, batch_size, n_epochs, lr, patience, step_size, device, save_path):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=step_size, gamma=0.1)\n",
    "\n",
    "    best_loss = math.inf\n",
    "    best_epoch = 0\n",
    "    k = 0\n",
    "\n",
    "    model_path = os.path.join(save_path, \"best_model_synthesis.pt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"\\n--- Epoch {epoch+1} ---\")\n",
    "        train_loss = train_epoch(model, optimizer, epoch, train_loader, device)\n",
    "        valid_loss = validation(model, valid_loader, device, epoch)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Valid Loss: {valid_loss:.4f}\")\n",
    "        scheduler.step()\n",
    "\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            best_epoch = epoch + 1\n",
    "            print(f\"Saving best model at epoch {best_epoch}\")\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "            # Generate a sample sequence\n",
    "            gen_seq, phi = generate_conditional_sequence(\n",
    "                model_path,\n",
    "                \"Hello Kaggle!\",\n",
    "                device,\n",
    "                train_loader.dataset.char_to_id,\n",
    "                train_loader.dataset.idx_to_char,\n",
    "                bias=10.0,\n",
    "                prime=False,\n",
    "                prime_seq=None,\n",
    "                real_text=None,\n",
    "                is_map=True,\n",
    "            )\n",
    "\n",
    "            # Plot attention heatmap inline\n",
    "            plt.figure(figsize=(12,4))\n",
    "            plt.imshow(phi, cmap=\"viridis\", aspect=\"auto\")\n",
    "            plt.colorbar()\n",
    "            plt.xlabel(\"time steps\")\n",
    "            plt.ylabel(\"characters\")\n",
    "            plt.show()\n",
    "\n",
    "            # Denormalize and plot strokes\n",
    "            gen_seq = data_denormalization(Global.train_mean, Global.train_std, gen_seq)\n",
    "            plot_stroke(gen_seq[0])  # inline plotting\n",
    "            k = 0\n",
    "        elif k > patience:\n",
    "            print(f\"Early stopping. Best epoch: {best_epoch}\")\n",
    "            break\n",
    "        else:\n",
    "            k += 1\n",
    "\n",
    "\n",
    "train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    batch_size,\n",
    "    n_epochs,\n",
    "    lr,\n",
    "    patience,\n",
    "    step_size,\n",
    "    device,\n",
    "    save_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"Handsysn.pth\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9221791,
     "sourceId": 14437230,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
